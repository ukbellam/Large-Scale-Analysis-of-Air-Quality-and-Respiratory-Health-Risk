{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dPXBtox7IUr1"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark numpy pandas pyarrow\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F, types as T\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, Bucketizer, ChiSqSelector\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.stat import Correlation, ChiSquareTest\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"SAT5165_AirQuality_Project\")\n",
        "         .getOrCreate())\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "DATA_PATH = \"/content/pollution_us_2000_2016.csv\"\n",
        "BASE_OUT = \"/content/output\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from functools import reduce\n",
        "\n",
        "DATA_PATH = \"/content/pollution_us_2000_2016.csv\"\n",
        "\n",
        "df = (spark.read\n",
        "      .option(\"header\", True)\n",
        "      .option(\"inferSchema\", True)\n",
        "      .csv(DATA_PATH))\n",
        "\n",
        "aqi_cols_all   = [\"NO2 AQI\",\"O3 AQI\",\"SO2 AQI\",\"CO AQI\",\"PM2.5 AQI\",\"PM10 AQI\"]\n",
        "means_pref_all = [\"NO2 Mean\",\"O3 Mean\",\"SO2 Mean\",\"CO Mean\",\"PM2.5 Mean\",\"PM10 Mean\"]\n",
        "\n",
        "aqi_present   = [c for c in aqi_cols_all   if c in df.columns]\n",
        "means_present = [c for c in means_pref_all if c in df.columns]\n",
        "\n",
        "if aqi_present:\n",
        "    df = df.withColumn(\n",
        "        \"AQI\",\n",
        "        F.array_max(F.array(*[F.col(c).cast(\"double\") for c in aqi_present]))\n",
        "    )\n",
        "else:\n",
        "    if not means_present:\n",
        "        raise ValueError(\"No AQI or pollutant mean columns found.\")\n",
        "    sum_expr = reduce(lambda a, b: a + b, [F.col(c).cast(\"double\") for c in means_present])\n",
        "    df = df.withColumn(\"AQI\", sum_expr)\n",
        "\n",
        "keep_cols = [\"AQI\",\"State\",\"County\",\"City\",\"Date Local\"] + means_present\n",
        "df = df.select(*[c for c in keep_cols if c in df.columns])\n",
        "\n",
        "df.write.mode(\"overwrite\").parquet(\"/content/output/00_raw_selected.parquet\")\n",
        "print(\"Saved → /content/output/00_raw_selected.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBh6a8MQJqwr",
        "outputId": "7a135527-bb7a-4783-cdd9-21ef0f2404cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved → /content/output/00_raw_selected.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(f\"{BASE_OUT}/00_raw_selected.parquet\")\n",
        "\n",
        "numeric_cols = [c for c, t in df.dtypes if t in (\"double\",\"float\",\"int\",\"bigint\")]\n",
        "global_medians = {}\n",
        "for c in numeric_cols:\n",
        "    q = df.approxQuantile(c, [0.5], 0.001)\n",
        "    global_medians[c] = float(q[0]) if q and q[0] is not None else None\n",
        "df = df.fillna(global_medians)\n",
        "\n",
        "if \"City\" in df.columns and \"Date_Local\" in df.columns:\n",
        "    w_roll = Window.partitionBy(\"State\",\"City\").orderBy(F.col(\"Date_Local\")).rowsBetween(-2,0)\n",
        "    for c in [\"NO2_Mean\",\"O3_Mean\",\"SO2_Mean\",\"CO_Mean\"]:\n",
        "        if c in df.columns:\n",
        "            df = df.withColumn(f\"{c}_sm\", F.avg(F.col(c)).over(w_roll))\n",
        "    feature_cols_clean = [f\"{c}_sm\" for c in [\"NO2_Mean\",\"O3_Mean\",\"SO2_Mean\",\"CO_Mean\"] if c in df.columns]\n",
        "else:\n",
        "    feature_cols_clean = [\"NO2_Mean\",\"O3_Mean\",\"SO2_Mean\",\"CO_Mean\"]\n",
        "\n",
        "df = df.withColumn(\"unhealthy_label\", (F.col(\"AQI\") >= 101).cast(\"int\"))\n",
        "df.write.mode(\"overwrite\").parquet(f\"{BASE_OUT}/10_clean.parquet\")\n",
        "print(\"Cleaned data saved at:\", f\"{BASE_OUT}/10_clean.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLWgK7fRKPvV",
        "outputId": "3d5d5b5e-f0da-431d-cfc9-bdfc9c65a249"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved at: /content/output/10_clean.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "df = spark.read.parquet(\"/content/output/10_clean.parquet\")\n",
        "\n",
        "# Prefer smoothed cols like \"NO2 Mean_sm\"; else use raw mean cols with spaces\n",
        "means_with_spaces = [\"NO2 Mean\",\"O3 Mean\",\"SO2 Mean\",\"CO Mean\",\"PM2.5 Mean\",\"PM10 Mean\"]\n",
        "smoothed_cols = [c for c in df.columns if c.endswith(\"_sm\")]\n",
        "feature_cols = smoothed_cols if smoothed_cols else [c for c in means_with_spaces if c in df.columns]\n",
        "\n",
        "if not feature_cols:\n",
        "    raise ValueError(\"No feature columns found for correlation.\")\n",
        "\n",
        "# Pairwise corr with AQI\n",
        "for c in feature_cols:\n",
        "    val = df.stat.corr(c, \"AQI\")\n",
        "    print(f\"Correlation between {c} and AQI = {val if val is not None else 'NA'}\")\n",
        "\n",
        "# Full feature correlation matrix\n",
        "corr_input = df.select(*feature_cols).dropna()\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_corr\")\n",
        "corr_df = assembler.transform(corr_input)\n",
        "corr_mat = Correlation.corr(corr_df, \"features_corr\", \"pearson\").head()[0]\n",
        "print(\"\\nCorrelation matrix (features):\")\n",
        "print(corr_mat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrzk46uiKcW0",
        "outputId": "88bdc1e8-92f7-4181-f7b0-e952a1ffe05a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between NO2 Mean and AQI = 0.3056180198257939\n",
            "Correlation between O3 Mean and AQI = 0.5227135895249344\n",
            "Correlation between SO2 Mean and AQI = 0.20492773215226473\n",
            "Correlation between CO Mean and AQI = 0.18195839797498403\n",
            "\n",
            "Correlation matrix (features):\n",
            "DenseMatrix([[ 1.        , -0.43265014,  0.34818603,  0.6418281 ],\n",
            "             [-0.43265014,  1.        , -0.11040144, -0.33942643],\n",
            "             [ 0.34818603, -0.11040144,  1.        ,  0.21521638],\n",
            "             [ 0.6418281 , -0.33942643,  0.21521638,  1.        ]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark numpy pandas pyarrow\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AQI_Regression\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "\n",
        "df = (spark.read\n",
        "      .option(\"header\", True)\n",
        "      .option(\"inferSchema\", True)\n",
        "      .csv(\"/content/pollution_us_2000_2016.csv\"))\n",
        "\n",
        "\n",
        "aqi_cols = [c for c in [\"NO2 AQI\",\"O3 AQI\",\"SO2 AQI\",\"CO AQI\",\"PM2.5 AQI\",\"PM10 AQI\"] if c in df.columns]\n",
        "if not aqi_cols:\n",
        "    raise ValueError(\"No AQI columns found (e.g., 'NO2 AQI'). Check the CSV headers.\")\n",
        "df = df.withColumn(\"AQI_label\", F.array_max(F.array(*[F.col(c).cast(\"double\") for c in aqi_cols]))).dropna(subset=[\"AQI_label\"])\n",
        "\n",
        "\n",
        "feature_cols = [c for c in [\"NO2 Mean\",\"O3 Mean\",\"SO2 Mean\",\"CO Mean\",\"PM2.5 Mean\",\"PM10 Mean\"] if c in df.columns]\n",
        "if not feature_cols:\n",
        "    raise ValueError(\"No pollutant mean columns found (e.g., 'NO2 Mean').\")\n",
        "df = df.dropna(subset=feature_cols).select(*(feature_cols + [\"AQI_label\"])).sample(False, 0.3, 42).cache()\n",
        "\n",
        "\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
        "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features_z\", withMean=True, withStd=True)\n",
        "lr = LinearRegression(featuresCol=\"features_z\", labelCol=\"AQI_label\", predictionCol=\"AQI_pred\", maxIter=50, regParam=0.1, elasticNetParam=0.2)\n",
        "\n",
        "model = Pipeline(stages=[assembler, scaler, lr]).fit(train)\n",
        "\n",
        "\n",
        "pred = model.transform(test).cache()\n",
        "rmse = RegressionEvaluator(labelCol=\"AQI_label\", predictionCol=\"AQI_pred\", metricName=\"rmse\").evaluate(pred)\n",
        "mae  = RegressionEvaluator(labelCol=\"AQI_label\", predictionCol=\"AQI_pred\", metricName=\"mae\").evaluate(pred)\n",
        "r2   = RegressionEvaluator(labelCol=\"AQI_label\", predictionCol=\"AQI_pred\", metricName=\"r2\").evaluate(pred)\n",
        "print(f\"RMSE: {rmse:.4f}  MAE: {mae:.4f}  R2: {r2:.4f}\")\n",
        "\n",
        "\n",
        "spark.createDataFrame([Row(feature=f, coefficient=float(c)) for f, c in zip(feature_cols, model.stages[-1].coefficients.toArray())]) \\\n",
        "     .write.mode(\"overwrite\").parquet(\"/content/aqi_regression_coefficients.parquet\")\n",
        "pred.select(*feature_cols, \"AQI_label\", \"AQI_pred\").write.mode(\"overwrite\").parquet(\"/content/aqi_regression_predictions.parquet\")\n",
        "spark.createDataFrame([(\"rmse\", float(rmse)), (\"mae\", float(mae)), (\"r2\", float(r2))], [\"metric\",\"value\"]) \\\n",
        "     .write.mode(\"overwrite\").parquet(\"/content/aqi_regression_metrics.parquet\")\n",
        "\n",
        "pred.select(*feature_cols, \"AQI_label\", \"AQI_pred\").show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0q_FGMOK3E8",
        "outputId": "916f9a82-6853-4c1a-e143-2e658adc0f7b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 11.6392  MAE: 7.5379  R2: 0.6266\n",
            "+--------+--------+--------+--------+---------+------------------+\n",
            "|NO2 Mean|O3 Mean |SO2 Mean|CO Mean |AQI_label|AQI_pred          |\n",
            "+--------+--------+--------+--------+---------+------------------+\n",
            "|0.0     |0.004917|2.913043|1.8875  |27.0     |3.686640716821884 |\n",
            "|0.0     |0.007542|2.173913|0.795833|31.0     |2.678932294151629 |\n",
            "|0.0     |0.007625|0.0     |0.879167|11.0     |1.9062933443701766|\n",
            "|0.0     |0.011542|0.130435|0.483333|20.0     |5.713916569286219 |\n",
            "|0.0     |0.013625|5.271429|0.379167|25.0     |10.929289100210333|\n",
            "|0.0     |0.014708|0.9625  |0.8125  |33.0     |11.62991792365656 |\n",
            "|0.0     |0.015444|0.0     |0.0     |14.0     |9.029265191031431 |\n",
            "|0.0     |0.0175  |0.516667|0.029167|17.0     |12.16461887776179 |\n",
            "|0.0     |0.019458|0.0     |0.0     |21.0     |14.38148039571611 |\n",
            "|0.0     |0.019708|0.0     |0.0     |19.0     |14.714827132430404|\n",
            "+--------+--------+--------+--------+---------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import Bucketizer, VectorAssembler\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "df = spark.read.parquet(f\"{BASE_OUT}/10_clean.parquet\")\n",
        "\n",
        "means_with_spaces = [\"NO2 Mean\",\"O3 Mean\",\"SO2 Mean\",\"CO Mean\",\"PM2.5 Mean\",\"PM10 Mean\"]\n",
        "\n",
        "\n",
        "smoothed_cols = [c for c in df.columns if c.endswith(\"_sm\")]\n",
        "expected_smoothed = [f\"{c}_sm\" for c in means_with_spaces]\n",
        "feature_cols = [c for c in smoothed_cols if c in expected_smoothed]\n",
        "\n",
        "\n",
        "if not feature_cols:\n",
        "    feature_cols = [c for c in means_with_spaces if c in df.columns]\n",
        "\n",
        "if not feature_cols:\n",
        "    raise ValueError(\"No pollutant feature columns found (neither *_sm nor raw means).\")\n",
        "\n",
        "\n",
        "if \"unhealthy_label\" not in df.columns:\n",
        "    df = df.withColumn(\"unhealthy_label\", (F.col(\"AQI\") >= 101).cast(\"int\"))\n",
        "else:\n",
        "    df = df.withColumn(\"unhealthy_label\", F.col(\"unhealthy_label\").cast(\"int\"))\n",
        "\n",
        "\n",
        "df = df.dropna(subset=feature_cols + [\"unhealthy_label\"])\n",
        "\n",
        "\n",
        "bins = 4\n",
        "bucketed_cols = []\n",
        "for c in feature_cols:\n",
        "    quants = df.approxQuantile(c, [i/bins for i in range(1, bins)], 0.01)\n",
        "    splits = [-float(\"inf\")] + quants + [float(\"inf\")]\n",
        "    b = Bucketizer(splits=splits, inputCol=c, outputCol=f\"{c}_bkt\").setHandleInvalid(\"keep\")\n",
        "    df = b.transform(df)\n",
        "    bucketed_cols.append(f\"{c}_bkt\")\n",
        "\n",
        "\n",
        "assembler = VectorAssembler(inputCols=bucketed_cols, outputCol=\"feat_cat\")\n",
        "chi_df = assembler.transform(df.select(\"unhealthy_label\", *bucketed_cols)).select(\"feat_cat\", \"unhealthy_label\")\n",
        "\n",
        "res = ChiSquareTest.test(chi_df, \"feat_cat\", \"unhealthy_label\").head()\n",
        "print(\"Chi-Square Test p-values:\", res.pValues)\n",
        "print(\"degreesOfFreedom:\", res.degreesOfFreedom)\n",
        "print(\"statistics:\", res.statistics)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd-xjRSAMM4Z",
        "outputId": "7178f0f3-9607-469c-ec9b-db465beead4c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chi-Square Test p-values: [0.0,0.0,0.0,0.0]\n",
            "degreesOfFreedom: [3, 3, 3, 3]\n",
            "statistics: [12374.238854643576,84583.46970261444,11305.897410927417,3187.5944494836167]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "\n",
        "BASE_OUT = \"/content/output\"\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SAT5165_PCA_Sucharitha\").getOrCreate()\n",
        "\n",
        "df = spark.read.parquet(f\"{BASE_OUT}/10_clean.parquet\")\n",
        "\n",
        "means_with_spaces = [\"NO2 Mean\",\"O3 Mean\",\"SO2 Mean\",\"CO Mean\",\"PM2.5 Mean\",\"PM10 Mean\"]\n",
        "smoothed_cols = [c for c in df.columns if c.endswith(\"_sm\")]\n",
        "expected_smoothed = [f\"{c}_sm\" for c in means_with_spaces]\n",
        "feature_cols = [c for c in smoothed_cols if c in expected_smoothed]\n",
        "if not feature_cols:\n",
        "    feature_cols = [c for c in means_with_spaces if c in df.columns]\n",
        "if not feature_cols:\n",
        "    raise ValueError(\"No feature columns found for PCA.\")\n",
        "\n",
        "for c in feature_cols:\n",
        "    df = df.withColumn(c, F.col(c).cast(DoubleType()))\n",
        "df = df.dropna(subset=feature_cols)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
        "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features_z\", withMean=True, withStd=True)\n",
        "k = min(4, len(feature_cols))\n",
        "pca = PCA(k=k, inputCol=\"features_z\", outputCol=\"pca_vec\")\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, scaler, pca])\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "ev = [float(x) for x in model.stages[-1].explainedVariance]\n",
        "print(\"Explained Variance by Principal Components:\", ev)\n",
        "\n",
        "scored = model.transform(df).withColumn(\"pca_arr\", vector_to_array(\"pca_vec\"))\n",
        "for i in range(len(ev)):\n",
        "    scored = scored.withColumn(f\"pc_{i+1}\", F.col(\"pca_arr\")[i])\n",
        "\n",
        "scored.select(*[f\"pc_{i+1}\" for i in range(len(ev))]).show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrmQ9mjlNl62",
        "outputId": "fe6a2251-38d6-4a4b-e8ed-a926a3c050b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance by Principal Components: [0.5257831826862004, 0.22542532144506988, 0.16650776678531745, 0.08228372908341236]\n",
            "+------------------+-------------------+-------------------+--------------------+\n",
            "|pc_1              |pc_2               |pc_3               |pc_4                |\n",
            "+------------------+-------------------+-------------------+--------------------+\n",
            "|1.7377327499127564|0.40158725623257435|0.10359616296385081|-0.21817895883172167|\n",
            "|1.7451441500301703|0.4032987370181092 |0.09608461043963472|-0.22604051704491213|\n",
            "|1.7419062347177334|0.3912312407875336 |0.10764207430078364|-0.22036476356978463|\n",
            "|1.7493176348351474|0.3929427215730684 |0.10013052177656755|-0.22822632178297508|\n",
            "|0.6286862157917354|0.07452214209596991|-0.6099443236202216|-0.2724119531224046 |\n",
            "+------------------+-------------------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}